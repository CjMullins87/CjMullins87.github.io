---
layout: post
title:  "/twaw {04/09/21}"
date:   2021-04-09 16:40:01 -0500
categories: jekyll update
---
## Problems I'm Solving
**Distilling meaningful strings from an array of similar strings/noisy substrings:**

>How do we meaningfully condense ['Campbells Soup', 'Campbells', 'Cambells Soup', 'Frittata', 'CAMPBELLS SOup'] into ['Campbells Soup']?

Not super sure how to proceed with this one. Current plan is definitely to clean the text data using standards from NLP (ie all lowercase, remove special characters, remove numbers). Best idea I can think of to proceed is to tokenize the strings and find meaningful tokens and bigrams. Haven't implemented a fix like this before, so it will require some research effort.

**Identifying substring patterns within an array of strings:**

>What patterns can we see in the set of strings ['animal', 'alpha', 'apple', 'arrange', 'array']?

We can see they all begin with A, but how do we programmatically address this? In my dataset a lot of the strings have the same length. So, I decided to do this by creating an `index_array` for each unique length in the array, then iterating over each string and updating the index array, such that the final output array shows the frequency with which a given character is in a given location.

This example is alphabetical, but my work is numerical, so the fxns below are only designed to work with numbers, but you could modify them to do the same thing for alphabetical characters.

## Tools I'm Using or Building
#### Using:
* [Fuzzywuzzy](https://pypi.org/project/fuzzywuzzy/) fuzzy string matching.

#### Building:
{% highlight python %}
def index_array(item):
    #Check if item is str
    if type(item) != str:

    #Convert to string if it's not    
        try:
            item = str(item)
        except:
            print('Type could not be converted to str')

    #Create array of zeros with dimensions rows x columns | n_rows(len(item) + 1)
    head = list(range(10))
    rows = len(item) + 1
    arr = np.zeros([rows, 10], dtype = 'int')
    arr[0] = head
    return arr

def index_arrays(batch):
    """
    Takes a list of integers or strings and returns a dictionary
    of unique string lengths and corresponding index arrays.
    """
    #Initialize the dictionary
    d = {}

    #Identify the unique string lengths in the list container
    #Converts integers to string, in the event a list of integers is passed
    for i in set([len(str(item)) for item in batch]):

        #Initializes the array object
        arr = np.zeros([i+1,10], dtype = 'int')
        arr[0] = list(range(10))

        #Adds array to dictionary
        d[i] = arr

    return d

def address_array(A, arr):
    """
    Passing (A, B) where A is a str/integer, and B is an appropriately
    shaped index array, the function will iterate over A and update the
    index array to show the index of the substring (row) and its 0-9 value (column)
    """

    #Convert to str
    for i in range(len(str(A))):

        #Determine address
        row = i+1
        column = int(str(A)[i])

        #Update array
        arr[row,column] += 1

    return arr

def freq_arrays(batch_list):
    """
    This function depends on index_arrays and address_array.
    It takes a list container of str/integers and outputs a dictionary
    of fully iterated address arrays.
    """

    #Initialize the dictionary
    d = index_arrays(batch_list)

    for item in batch_list:
        #Check if item is str
        if type(item) != str:

        #Convert to string if it's not    
            try:
                item = str(item)
            except:
                print('Type could not be converted to str')

        #Store len(item) and call arr from dictionary
        l = len(item)
        arr = d[l]

        #Update address_array
        arr = address_array(item, arr)

        #Store updated array
        d[l] = arr

    return d
{% endhighlight %}

Final result of this work is the `freq_arrays` fxn, which works like this:

* Find the set of unique string lengths
* Build an index array for each length, and store the index array in a dictionary
* Then, for each item in the list:
  * Check its length and call the appropriate array
  * Iterate over the string and mark the appropriate index positions
  * Store the updated array in the dictionary and then move on

You can see it's built on two other helper functions that came before it - this basically shows you how I got to the final point as an iterative process. Overall, this is a very useful function for my purposes, in that it will visually display positional data about the set of strings in a way that is both intuitive and easy to read for stakeholders.

![Sample output](/images/photo_2021-04-09_14-35-26.jpg)

Plus, not sure I'll have to use it for something like this later, but it's a great way to store information about a set of n shaped numerical strings. I can store all the information needed to recreate the entire list without needing to take up all the memory resources of a python list.

## The Next Step
* Definitely do some high level research on NLP techniques. I think tokens and bigrams will be the way to go. If I can find bigrams and tokens that correlate to teach other, that might be a good way to condense.
* Eventually these two techniques are going to feed into each other, so I'll need to figure out whether to pipeline `numerical patterns -> string patterns` or `string patterns -> numerical patterns`
