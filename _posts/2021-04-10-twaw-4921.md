---
layout: post
title:  "/twaw {04/09/21}"
date:   2021-04-09 16:40:01 -0500
categories: jekyll update
---
## Problems I'm Solving
**Distilling meaningful strings from an array of similar strings/noisy substrings:**

>How do we meaningfully condense ['Campbells Soup', 'Campbells', 'Cambells Soup', 'Frittata', 'CAMPBELLS SOup'] into ['Campbells Soup']?

Not super sure how to proceed with this one. Current plan is definitely to clean the text data using standards from NLP (ie all lowercase, remove special characters, remove numbers). Best idea I can think of to proceed is to tokenize the strings and find meaningful tokens and bigrams. Haven't implemented a fix like this before, so it will require some research effort.

**Identifying substring patterns within an array of strings:**

>What patterns can we see in the set of strings ['animal', 'alpha', 'apple', 'arrange', 'array']?

We can see they all begin with A, but how do we programmatically address this? In my dataset a lot of the strings have the same length. So, I decided to do this by creating an `index_array` for each unique length in the array, then iterating over each string and updating the index array, such that the final output array shows the frequency with which a given character is in a given location.

This example is alphabetical, but my work is numerical, so the fxns below are only designed to work with numbers, but you could modify them to do the same thing for alphabetical characters.

## Tools I'm Using or Building
#### Using:
* [Fuzzywuzzy](https://pypi.org/project/fuzzywuzzy/) fuzzy string matching.

#### Building:
`def index_array(integer):
    #Check if invoice is str
    if type(integer) != str:
    
    #Convert to string if it's not    
        try:
            integer = str(integer)
        except:
            print('Type could not be converted to str')
    
    #Create array of zeros with dimensions rows x columns | n_rows(len(integer) + 1)
    head = list(range(10))
    rows = len(integer) + 1
    arr = np.zeros([rows, 10], dtype = 'int')
    arr[0] = head
    return arr

def index_arrays(batch):
    """
    Takes a list of integers or strings and returns a dictionary
    of unique string lengths and corresponding index arrays.
    """
    #Initialize the dictionary
    d = {}
    
    #Identify the unique string lengths in the list container
    #Converts integers to string, in the event a list of integers is passed
    for i in set([len(str(item)) for item in batch]):
        
        #Initializes the array object
        arr = np.zeros([i+1,10], dtype = 'int')
        arr[0] = list(range(10))
        
        #Adds array to dictionary
        d[i] = arr
    
    return d

def address_array(A, arr):
    """
    Passing (A, B) where A is a str/integer, and B is an appropriately
    shaped index array, the function will iterate over A and update the
    index array to show the index of the substring (row) and its 0-9 value (column)
    """
    
    #Convert to str
    for i in range(len(str(A))):
        
        #Determine address
        row = i+1
        column = int(str(A)[i])
        
        #Update array
        arr[row,column] += 1
    
    return arr

def freq_arrays(batch_list):
    """
    This function depends on index_arrays and address_array.
    It takes a list container of str/integers and outputs a dictionary
    of fully iterated address arrays.
    """
    
    #Initialize the dictionary
    d = index_arrays(batch_list)

    for item in batch_list:
        #Check if item is str
        if type(item) != str:
    
        #Convert to string if it's not    
            try:
                item = str(item)
            except:
                print('Type could not be converted to str')
             
        #Store len(item) and call arr from dictionary
        l = len(item)
        arr = d[l]
    
        #Update address_array
        arr = address_array(item, arr)
    
        #Store updated array
        d[l] = arr
    
    return d`

## The Next Step
* Definitely do some high level research on NLP techniques. I think tokens and bigrams will be the way to go. If I can find bigrams and tokens that correlate to teach other, that might be a good way to condense.
* Eventually these two techniques are going to feed into each other, so I'll need to figure out whether to pipeline `numerical patterns -> string patterns` or `string patterns -> numerical patterns`
