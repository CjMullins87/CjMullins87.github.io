---
layout: post
title:  "/twaw {04/16/21}"
date:   2021-04-16 16:40:01 -0500
categories: jekyll update
---
## Problems I'm Solving
**Meaningfully grouping similar strings:**

>How do we meaningfully condense ['Campbells Soup', 'Campbells', 'Cambells Soup', 'Frittata', 'CAMPBELLS SOup'] into ['Campbells Soup']?

After several days of research, I found that a lot of the guidance on fuzzy string clustering came down to normalizing the strings (I did this with regex), vectorizing them in a matrix, and then using a clustering algorithm. This ran into several issues:

1. What's the best way to vectorize?
2. What's the best clustering algorithm?

In this case, I got the best results with tf-idf vectorization, which helps identify important words or combinations of words by regularizing them according to their frequency both within a 'document' and between 'documents'. In this case, our documents are the multi-word strings in the array.

What happens is tf-idf will assign a value to the individual words in a way that emphasizes what makes the strings different from each other so that it's easier to see where the differences are. E.g. if all the strings say "Campbells soup", and another few strings say "Campbells condensed soup", then, because the word "condensed" happens less frequently between all the documents, it is given a higher weight. This makes sense in an intuitive way, in that what is different is often what is significant - i.e. it's interesting to find a diamond in a coal mine, but less interesting to find a diamond in a diamond mine.

A lot of people recommended k-means for clustering, which I've used before, but I couldn't use this time since I'll (1) be doing blind processing once I'm out of my test case, and (2) have no way of knowing how many good clusters there are. I needed something that would automatically determine how many clusters there are, which meant I needed DBSCAN. DBSCAN is ideal because it determines the number of clusters once you specify the *n* minimum units in a cluster, and the *e* distance within which to look for neighbors.

There are [several ways](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html#sklearn.metrics.pairwise_distances) to measure *e*, but I got the best results with cosine distance.

**Identifying patterns with substrings:**

>In the set ['E621', 'E456', 'E789', 'E980', 'E134', 'E522'], how do we identify that E in position 1 is a true pattern, and not just random?

To do this, I basically feed the freq_arrays from the last time (there's an updated version here that can handle alphanumeric characters) through a new function, `general_form`, which finds the maximum
position count from one our frequency arrays, and then calculates the total probability that that count would be met or exceeded if character selection was completely random. If the probability that this is random is <0.001, it gets output in the general form.

This probability is calculated using a binomial distribution, since, in a randomly generated string:

1. Each character is independently selected
2. Each character either is (1) or isn't (0) in a position
3. Each character has the same *p* probability of being selected.

So, determining the probability of our observed outcome *r* can tell us whether the string really is randomly generated, or it follows a set order.

![Result](\images\41621_generalform.PNG)

This is probably my favorite function I've built so far. Another nifty feature is that it takes into account whether you're feeding a series of numeric strings or alphanumeric strings! I had to build this in since it's possible to run into a series of either, and the probability spaces within a numeric string are much larger than those in an alphanumeric string (*p = 1/10* vs *p = 1/36*).

## Tools I'm Using or Building
#### Using:
* Regex
* SciKitLearn:  tfidfvectorizer, DBSCAN
* Scipy:  binom

#### Building:
{% highlight python %}
import numpy as np
from scipy.stats import binom

def freq_arrays(strings_list):
    """
    This function takes in a list of alphanumerical objects and returns unique arrays for
    each string length. Arrays demonstrate the frequency with which alphanumeric characters
    are in each position, where [row] is the index position within the string and [column]
    is the alphanumeric character. Columns [0-9] are numbers 0-9, and columns 10-36 are
    letters a-z.
    """
    #Takes list of integers or strings and returns a dictionary
    #of unique string lengths and corresponding index arrays.

    #Initialize the dictionary
    d = {}

    #Identify the unique string lengths in the list container
    #Converts integers to string, in the event a list of integers is passed
    for i in set([len(str(sstring)) for sstring in strings_list]):

        #Initializes the array object, #adds to dictionary
        arr = np.zeros((i,36), dtype = 'int')
        d[i] = arr


    #Represent the frequency with which a pattern exists
    for sstring in strings_list:

        #QC on sstring - avoid special characters/AI misreads
        if sstring.isalnum() == True:

            #Call the correct array
            l = len(sstring)
            arr = d[l]

            #Update the array
            for index, character in enumerate(sstring):
                if character.isnumeric() == True:
                    row = index
                    column = int(character)
                    arr[row][column] += 1
                else:
                    row = index
                    column = ord(character.lower())-87
                    arr[row][column] += 1

            #Store the array
            d[l] = arr

        else:
            continue

    return d

    def general_form(array):
        """
        This fxn uses scipy.stats to calculate the probability that an observed
        maximum frequency is random and generates a 'general form' for the string
        if the pattern meets a confidence threshhold.
        """
        #Define binomial params as fxns of array space

        #If string is alphanumeric (mixed numbers and letters) it will have values
        #in the alpha columns [10:36]. Setting threshhold at 2 so that the AI has to
        #make two mistakes in interpretation to trigger this calculation.
        if array[:,10:36].sum() >= 2:
            n = array[0].sum()
            p = 1/36
            k = array.max()
            r = 1-binom.cdf(k, n, p)+binom.pmf(k, n, p)

        #If string is numeric (only numbers) it will only have values in the number
        #columns [0:9], which means the alpha columns should be equal to 0. Setting
        #threshhold at 1 so that the the AI can make 1 mistake.
        else:
            n = array[0].sum()
            p = 1/10
            k = array.max()
            r = 1-binom.cdf(k, n, p)+binom.pmf(k, n, p)

        #Generate stock form for string
        form = 'X '*array.shape[0]
        form = form.split()

        #Special case of high confidence
        if r < 0.001:

            #Retrieve position of k
            rows, columns = np.where(array == k)

            #Translate K
            for index, row in enumerate(rows):
                if columns[index] >= 10:
                    form[row] = chr(columns[index]+87)
                else:
                    form[row] = str(columns[index])

        return ''.join(form)
{% endhighlight %}

## The Next Step
* At this point, it's just assembling all my components into a single pipeline.
